<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Evelyn Metzger">
<meta name="dcterms.date" content="2025-02-21">
<meta name="description" content="This post shows you how to generate OME-TIFF files from napari-cosmx-generated Zarr stores.">

<title>Exporting napari-cosmx Zarr stores to OME-TIFF – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../assets/logo_Bruker_black.png" rel="icon" type="image/png">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-3a01e2046221230fdceeea94b1ec5d67.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark-a4a11d514c7d463668e07712114998e6.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-1d786c23d0c3165c27120f995b1b0186.min.css" rel="stylesheet" append-hash="true" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-29W4MW0Y2W"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-29W4MW0Y2W', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Exporting `napari-cosmx` Zarr stores to OME-TIFF">
<meta name="citation_author" content="Evelyn Metzger">
<meta name="citation_publication_date" content="2025-02-21">
<meta name="citation_cover_date" content="2025-02-21">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-02-21">
<meta name="citation_fulltext_html_url" content="https://nanostring-biostats.github.io/CosMx-Analysis-Scratch-Space/posts/napari-ome-tiff/cosmx-ome-tiff.html">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Insitutype: Likelihood-based cell typing for single cell spatial transcriptomics;,citation_abstract=Accurate cell typing is fundamental to analysis of spatial single-cell transcriptomics, but legacy scRNA-seq algorithms can underperform in this new type of data. We have developed a cell typing algorithm, Insitutype, designed for statistical and computational efficiency in spatial transcriptomics data.Insitutype is based on a likelihood model that weighs the evidence from every expression value, extracting all the information available in each cell’s expression profile. This likelihood model underlies a Bayes classifier for supervised cell typing, and an Expectation-Maximization algorithm for unsupervised and semi-supervised clustering. Insitutype also leverages alternative data types collected in spatial studies, such as cell images and spatial context, by using them to inform prior probabilities of cell type calls. We demonstrate rapid clustering of millions of cells and accurate fine-grained cell typing of kidney and non-small cell lung cancer samples.Competing Interest StatementPD, EZ, ZY, DR, MG, ZR, TKK, SH, DH and JMB are current/former employees and shareholders of NanoString.;,citation_author=Patrick Danaher;,citation_author=Edward Zhao;,citation_author=Zhi Yang;,citation_author=David Ross;,citation_author=Mark Gregory;,citation_author=Zach Reitz;,citation_author=Tae K. Kim;,citation_author=Sarah Baxter;,citation_author=Shaun Jackson;,citation_author=Shanshan He;,citation_author=Dave Henderson;,citation_author=Joseph M. Beechem;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/10/21/2022.10.19.512902;,citation_doi=10.1101/2022.10.19.512902;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
<meta name="citation_reference" content="citation_title=Analytic pearson residuals for normalization of single-cell RNA-seq UMI data;,citation_author=Jan Lause;,citation_author=Philipp Berens;,citation_author=Dmitry Kobak;,citation_publication_date=2021-09;,citation_cover_date=2021-09;,citation_year=2021;,citation_doi=10.1186/s13059-021-02451-7;,citation_issn=1474-760X;,citation_volume=22;,citation_journal_title=Genome Biology;">
<meta name="citation_reference" content="citation_title=High-plex imaging of RNA and proteins at subcellular resolution in fixed tissue by spatial molecular imaging.;,citation_abstract=Resolving the spatial distribution of RNA and protein in tissues at subcellular resolution is a challenge in the field of spatial biology. We describe spatial molecular imaging, a system that measures RNAs and proteins in intact biological samples at subcellular resolution by performing multiple cycles of nucleic acid hybridization of fluorescent molecular barcodes. We demonstrate that spatial molecular imaging has high sensitivity (one or two copies per cell) and very low error rate (0.0092 false calls per cell) and background (&nbsp;0.04 counts per cell). The imaging system generates three-dimensional, super-resolution localization of analytes at &nbsp;2 million cells per sample. Cell segmentation is morphology based using antibodies, compatible with formalin-fixed, paraffin-embedded samples. We measured multiomic data (980 RNAs and 108 proteins) at subcellular resolution in formalin-fixed, paraffin-embedded tissues (nonsmall cell lung and breast cancer) and identified &amp;amp;amp;gt;18 distinct cell types, ten unique tumor microenvironments and 100 pairwise ligand-receptor interactions. Data on &amp;gt;800,000 single cells and &nbsp;260 million transcripts can be accessed at http://nanostring.com/CosMx-dataset .;,citation_author=Shanshan He;,citation_author=Ruchir Bhatt;,citation_author=Carl Brown;,citation_author=Emily A Brown;,citation_author=Derek L Buhr;,citation_author=Kan Chantranuvatana;,citation_author=Patrick Danaher;,citation_author=Dwayne Dunaway;,citation_author=Ryan G Garrison;,citation_author=Gary Geiss;,citation_author=Mark T Gregory;,citation_author=Margaret L Hoang;,citation_author=Rustem Khafizov;,citation_author=Emily E Killingbeck;,citation_author=Dae Kim;,citation_author=Tae Kyung Kim;,citation_author=Youngmi Kim;,citation_author=Andrew Klock;,citation_author=Mithra Korukonda;,citation_author=Alecksandr Kutchma;,citation_author=Zachary R Lewis;,citation_author=Yan Liang;,citation_author=Jeffrey S Nelson;,citation_author=Giang T Ong;,citation_author=Evan P Perillo;,citation_author=Joseph C Phan;,citation_author=Tien Phan-Everson;,citation_author=Erin Piazza;,citation_author=Tushar Rane;,citation_author=Zachary Reitz;,citation_author=Michael Rhodes;,citation_author=Alyssa Rosenbloom;,citation_author=David Ross;,citation_author=Hiromi Sato;,citation_author=Aster W Wardhani;,citation_author=Corey A Williams-Wietzikoski;,citation_author=Lidan Wu;,citation_author=Joseph M Beechem;,citation_publication_date=2022-12;,citation_cover_date=2022-12;,citation_year=2022;,citation_doi=10.1038/s41587-022-01483-z;,citation_issn=1546-1696;,citation_pmid=36203011;,citation_volume=40;,citation_journal_title=Nature biotechnology;">
<meta name="citation_reference" content="citation_title=Squidpy: A scalable framework for spatial omics analysis;,citation_abstract=Spatial omics data are advancing the study of tissue organization and cellular communication at an unprecedented scale. Flexible tools are required to store, integrate and visualize the large diversity of spatial omics data. Here, we present Squidpy, a Python framework that brings together tools from omics and image analysis to enable scalable description of spatial molecular data, such as transcriptome or multivariate proteins. Squidpy provides efficient infrastructure and numerous analysis methods that allow to efficiently store, manipulate and interactively visualize spatial omics data. Squidpy is extensible and can be interfaced with a variety of already existing libraries for the scalable analysis of spatial omics data.;,citation_author=Giovanni Palla;,citation_author=Hannah Spitzer;,citation_author=Michal Klein;,citation_author=David Fischer;,citation_author=Anna Christina Schaar;,citation_author=Louis Benedikt Kuemmerle;,citation_author=Sergei Rybakov;,citation_author=Ignacio L. Ibarra;,citation_author=Olle Holmberg;,citation_author=Isaac Virshup;,citation_author=Mohammad Lotfollahi;,citation_author=Sabrina Richter;,citation_author=Fabian J. Theis;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_doi=10.1038/s41592-021-01358-2;,citation_issn=1548-7091;,citation_volume=19;,citation_journal_title=Nature Methods;">
<meta name="citation_reference" content="citation_title=Dictionary learning for integrative, multimodal and scalable single-cell analysis;,citation_author=Yuhan Hao;,citation_author=Tim Stuart;,citation_author=Madeline H. Kowalski;,citation_author=Saket Choudhary;,citation_author=Paul Hoffman;,citation_author=Austin Hartman;,citation_author=Avi Srivastava;,citation_author=Gesmira Molla;,citation_author=Shaista Madad;,citation_author=Carlos Fernandez-Granda;,citation_author=Rahul Satija;,citation_publication_date=2024-02;,citation_cover_date=2024-02;,citation_year=2024;,citation_doi=10.1038/s41587-023-01767-y;,citation_issn=1087-0156;,citation_volume=42;,citation_journal_title=Nature Biotechnology;">
<meta name="citation_reference" content="citation_title=Giotto: A toolbox for integrative analysis and visualization of spatial expression data;,citation_abstract=Spatial transcriptomic and proteomic technologies have provided new opportunities to investigate cells in their native microenvironment. Here we present Giotto, a comprehensive and open-source toolbox for spatial data analysis and visualization. The analysis module provides end-to-end analysis by implementing a wide range of algorithms for characterizing tissue composition, spatial expression patterns, and cellular interactions. Furthermore, single-cell RNAseq data can be integrated for spatial cell-type enrichment analysis. The visualization module allows users to interactively visualize analysis outputs and imaging features. To demonstrate its general applicability, we apply Giotto to a wide range of datasets encompassing diverse technologies and platforms.;,citation_author=Ruben Dries;,citation_author=Qian Zhu;,citation_author=Rui Dong;,citation_author=Chee-Huat Linus Eng;,citation_author=Huipeng Li;,citation_author=Kan Liu;,citation_author=Yuntian Fu;,citation_author=Tianxiao Zhao;,citation_author=Arpan Sarkar;,citation_author=Feng Bao;,citation_author=Rani E. George;,citation_author=Nico Pierson;,citation_author=Long Cai;,citation_author=Guo-Cheng Yuan;,citation_publication_date=2021-12;,citation_cover_date=2021-12;,citation_year=2021;,citation_doi=10.1186/s13059-021-02286-2;,citation_issn=1474-760X;,citation_volume=22;,citation_journal_title=Genome Biology;">
<meta name="citation_reference" content="citation_title=SCANPY: Large-scale single-cell gene expression data analysis;,citation_abstract=Scanpy is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (https://github.com/theislab/Scanpy). Along with Scanpy, we present AnnData, a generic class for handling annotated data matrices (https://github.com/theislab/anndata).;,citation_author=F Alexander Wolf;,citation_author=Philipp Angerer;,citation_author=Fabian J Theis;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.1186/s13059-017-1382-0;,citation_doi=10.1186/s13059-017-1382-0;,citation_issn=1474-760X;,citation_volume=19;,citation_journal_title=Genome Biology;">
<meta name="citation_reference" content="citation_title=Differential expression and clinical significance of COX6C in human diseases.;,citation_abstract=Mitochondria, independent double-membrane organelles, are intracellular power plants that feed most eukaryotic cells with the ATP produced via the oxidative phosphorylation (OXPHOS). Consistently, cytochrome c oxidase (COX) catalyzes the electron transfer chain’s final step. Electrons are transferred from reduced cytochrome c to molecular oxygen and play an indispensable role in oxidative phosphorylation of cells. Cytochrome c oxidase subunit 6c (COX6C) is encoded by the nuclear genome in the ribosome after translation and is transported to mitochondria via different pathways, and eventually forms the COX complex. In recent years, many studies have shown the abnormal level of COX6C in familial hypercholesterolemia, chronic kidney disease, diabetes, breast cancer, prostate cancer, uterine leiomyoma, follicular thyroid cancer, melanoma tissues, and other conditions. Its underlying mechanism may be related to the cellular oxidative phosphorylation pathway in tissue injury disease. Here reviews the varied function of COX6C in non-tumor and tumor diseases.;,citation_author=Bi-Xia Tian;,citation_author=Wei Sun;,citation_author=Shu-Hong Wang;,citation_author=Pei-Jun Liu;,citation_author=Yao-Chun Wang;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_issn=1943-8141;,citation_pmid=33527004;,citation_volume=13;,citation_journal_title=American journal of translational research;">
<meta name="citation_reference" content="citation_title=BIDCell: Biologically-informed self-supervised learning for segmentation of subcellular spatial transcriptomics data;,citation_abstract=Recent advances in subcellular imaging transcriptomics platforms have enabled high-resolution spatial mapping of gene expression, while also introducing significant analytical challenges in accurately identifying cells and assigning transcripts. Existing methods grapple with cell segmentation, frequently leading to fragmented cells or oversized cells that capture contaminated expression. To this end, we present BIDCell, a self-supervised deep learning-based framework with biologically-informed loss functions that learn relationships between spatially resolved gene expression and cell morphology. BIDCell incorporates cell-type data, including single-cell transcriptomics data from public repositories, with cell morphology information. Using a comprehensive evaluation framework consisting of metrics in five complementary categories for cell segmentation performance, we demonstrate that BIDCell outperforms other state-of-the-art methods according to many metrics across a variety of tissue types and technology platforms. Our findings underscore the potential of BIDCell to significantly enhance single-cell spatial expression analyses, enabling great potential in biological discovery.;,citation_author=Xiaohang Fu;,citation_author=Yingxin Lin;,citation_author=David M. Lin;,citation_author=Daniel Mechtersheimer;,citation_author=Chuhan Wang;,citation_author=Farhan Ameen;,citation_author=Shila Ghazanfar;,citation_author=Ellis Patrick;,citation_author=Jinman Kim;,citation_author=Jean Y. H. Yang;,citation_publication_date=2024-01;,citation_cover_date=2024-01;,citation_year=2024;,citation_doi=10.1038/s41467-023-44560-w;,citation_issn=2041-1723;,citation_volume=15;,citation_journal_title=Nature Communications;">
<meta name="citation_reference" content="citation_title=Mitigating autocorrelation during spatially resolved transcriptomics data analysis;,citation_abstract=Several computational methods have recently been developed for characterizing molecular tissue regions in spatially resolved transcriptomics (SRT) data. However, each method fundamentally relies on spatially smoothing transcriptomic features across neighboring cells. Here, we demonstrate that smoothing increases autocorrelation between neighboring cells, causing latent space to encode physical adjacency rather than spatial transcriptomic patterns. We find that randomly sub-sampling neighbors before smoothing mitigates autocorrelation, improving the performance of existing methods and further enabling a simpler, more efficient approach that we call spatial integration (SPIN). SPIN leverages the conventional single-cell toolkit, yielding spatial analogies to each tool: clustering identifies molecular tissue regions; differentially expressed gene analysis calculates region marker genes; trajectory inference reveals continuous, molecularly defined ana tomical axes; and integration allows joint analysis across multiple SRT datasets, regardless of tissue morphology, spatial resolution, or experimental technology. We apply SPIN to SRT datasets from mouse and marmoset brains to calculate shared and species-specific region marker genes as well as a molecularly defined neocortical depth axis along which several genes and cell types differ across species.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Kamal Maher;,citation_author=Morgan Wu;,citation_author=Yiming Zhou;,citation_author=Jiahao Huang;,citation_author=Qiangge Zhang;,citation_author=Xiao Wang;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2023/07/02/2023.06.30.547258;,citation_doi=10.1101/2023.06.30.547258;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
<meta name="citation_reference" content="citation_title=Differential expression analysis for spatially correlated data;,citation_abstract=Differential expression is a key application of imaging spatial transcriptomics, moving analysis beyond cell type localization to examining cell state responses to microenvironments. However, spatial data poses new challenges to differential expression: segmentation errors cause bias in fold-change estimates, and correlation among neighboring cells leads standard models to inflate statistical significance. We find that ignoring these issues can result in considerable false discoveries that greatly outnumber true findings. We present a suite of solutions to these fundamental challenges, and implement them in the R package smiDE.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Ana Gabriela Vasconcelos;,citation_author=Daniel McGuire;,citation_author=Noah Simon;,citation_author=Patrick Danaher;,citation_author=Ali Shojaie;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2024/08/06/2024.08.02.606405;,citation_doi=10.1101/2024.08.02.606405;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
<meta name="citation_reference" content="citation_title=Using transcripts to refine image based cell segmentation with FastReseg;,citation_abstract=Spatial transcriptomics (ST) faces persistent challenges in cell segmentation accuracy, which can bias biological interpretations in a spatial-dependent way. FastReseg introduces a novel algorithm that refines inaccuracies in existing image-based segmentations using transcriptomic data, without radically redefining cell boundaries. By combining image-based information with 3D transcriptomic precision, FastReseg enhances segmentation accuracy. Its key innovation, a transcript scoring system based on log-likelihood ratios, facilitates the quick identification and correction of spatial doublets caused by cell proximity or overlap in 2D. FastReseg reduces circularity in boundary derivation, and addresses computational challenges with a modular workflow designed for large datasets. The algorithm’s modularity allows for seamless optimization and integration of advancements in segmentation technology. FastReseg provides a scalable, efficient solution to improve the quality and interpretability of ST data, ensuring compatibility with evolving segmentation methods and enabling more accurate biological insights.;,citation_author=Lidan Wu;,citation_author=Joseph M. Beechem;,citation_author=Patrick Danaher;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://doi.org/10.1038/s41598-025-08733-5;,citation_issue=1;,citation_doi=10.1038/s41598-025-08733-5;,citation_issn=2045-2322;,citation_volume=15;,citation_journal_title=Scientific Reports;">
<meta name="citation_reference" content="citation_title=Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization;,citation_author=Martin Slawski;,citation_author=Matthias Hein;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_fulltext_html_url=https://doi.org/10.1214/13-EJS868;,citation_issue=none;,citation_doi=10.1214/13-EJS868;,citation_volume=7;,citation_journal_title=Electronic Journal of Statistics;,citation_publisher=Institute of Mathematical Statistics; Bernoulli Society;">
<meta name="citation_reference" content="citation_title=Identifiability of nonparametric mixture models and Bayes optimal clustering;,citation_author=Bryon Aragam;,citation_author=Chen Dan;,citation_author=Eric P. Xing;,citation_author=Pradeep Ravikumar;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://doi.org/10.1214/19-AOS1887;,citation_issue=4;,citation_doi=10.1214/19-AOS1887;,citation_volume=48;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Expectation-maximization for sparse and non-negative PCA;,citation_abstract=We study the problem of finding the dominant eigenvector of the sample covariance matrix, under additional constraints on the vector: a cardinality constraint limits the number of non-zero elements, and non-negativity forces the elements to have equal sign. This problem is known as sparse and non-negative principal component analysis (PCA), and has many applications including dimensionality reduction and feature selection. Based on expectation-maximization for probabilistic PCA, we present an algorithm for any combination of these constraints. Its complexity is at most quadratic in the number of dimensions of the data. We demonstrate significant improvements in performance and computational efficiency compared to other constrained PCA algorithms, on large data sets from biology and computer vision. Finally, we show the usefulness of non-negative sparse PCA for unsupervised feature selection in a gene clustering task.;,citation_author=Christian D. Sigg;,citation_author=Joachim M. Buhmann;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_fulltext_html_url=https://doi.org/10.1145/1390156.1390277;,citation_doi=10.1145/1390156.1390277;,citation_isbn=9781605582054;,citation_conference_title=Proceedings of the 25th international conference on machine learning;,citation_conference=Association for Computing Machinery;,citation_series_title=ICML ’08;">
<meta name="citation_reference" content="citation_title=Fast, sensitive and accurate integration of single-cell data with harmony;,citation_author=Ilya Korsunsky;,citation_author=Nghia Millard;,citation_author=Jean Fan;,citation_author=Kamil Slowikowski;,citation_author=Fan Zhang;,citation_author=Kevin Wei;,citation_author=Yuriy Baglaenko;,citation_author=Michael Brenner;,citation_author=Po-ru Loh;,citation_author=Soumya Raychaudhuri;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=12;,citation_volume=16;,citation_journal_title=Nature methods;,citation_publisher=Nature Publishing Group US New York;">
<meta name="citation_reference" content="citation_title=Batch correction methods used in single cell RNA-sequencing analyses are often poorly calibrated;,citation_author=Sindri Emmanúel Antonsson;,citation_author=Páll Melsted;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
<meta name="citation_reference" content="citation_title=A benchmark of batch-effect correction methods for single-cell RNA sequencing data;,citation_author=Hoa Thi Nhu Tran;,citation_author=Kok Siong Ang;,citation_author=Marion Chevrier;,citation_author=Xiaomeng Zhang;,citation_author=Nicole Yee Shin Lee;,citation_author=Michelle Goh;,citation_author=Jinmiao Chen;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_volume=21;,citation_journal_title=Genome biology;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Current best practices in single-cell RNA-seq analysis: A tutorial;,citation_author=Malte D Luecken;,citation_author=Fabian J Theis;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=6;,citation_volume=15;,citation_journal_title=Molecular systems biology;">
<meta name="citation_reference" content="citation_title=From louvain to leiden: Guaranteeing well-connected communities;,citation_author=Vincent A Traag;,citation_author=Ludo Waltman;,citation_author=Nees Jan Van Eck;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_issue=1;,citation_volume=9;,citation_journal_title=Scientific reports;,citation_publisher=Nature Publishing Group;">
<meta name="citation_reference" content="citation_title=Uniform manifold approximation and projection (UMAP) and its variants: Tutorial and survey;,citation_author=Benyamin Ghojogh;,citation_author=Ali Ghodsi;,citation_author=Fakhri Karray;,citation_author=Mark Crowley;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2109.02508;">
<meta name="citation_reference" content="citation_title=Cell segmentation in imaging-based spatial transcriptomics;,citation_author=Vladimir Petukhov;,citation_author=Rui J. Xu;,citation_author=Ruslan A. Soldatov;,citation_author=Pietro Cadinu;,citation_author=Konstantin Khodosevich;,citation_author=Jeffrey R. Moffitt;,citation_author=Peter V. Kharchenko;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://doi.org/10.1038/s41587-021-01044-w;,citation_doi=10.1038/s41587-021-01044-w;,citation_journal_title=Nature Biotechnology;">
<meta name="citation_reference" content="citation_title=Cell simulation as cell segmentation;,citation_author=D. C. Jones;,citation_author=A. E. Elz;,citation_author=A. Hadadianpour;,citation_author=others;,citation_publication_date=2025;,citation_cover_date=2025;,citation_year=2025;,citation_fulltext_html_url=https://doi.org/10.1038/s41592-025-02697-0;,citation_doi=10.1038/s41592-025-02697-0;,citation_journal_title=Nature Methods;">
<meta name="citation_reference" content="citation_title=FICTURE: Scalable segmentation-free analysis of submicron-resolution spatial transcriptomics;,citation_author=Yichen Si;,citation_author=ChangHee Lee;,citation_author=Yongha Hwang;,citation_author=Jeong H. Yun;,citation_author=Weiqiu Cheng;,citation_author=Chun-Seok Cho;,citation_author=Miguel Quiros;,citation_author=Asma Nusrat;,citation_author=Weizhou Zhang;,citation_author=Goo Jun;,citation_author=Sebastian Zöllner;,citation_author=Jun Hee Lee;,citation_author=Hyun Min Kang;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://doi.org/10.1038/s41592-024-02415-2;,citation_issue=10;,citation_doi=10.1038/s41592-024-02415-2;,citation_issn=1548-7105;,citation_volume=21;,citation_journal_title=Nature Methods;">
<meta name="citation_reference" content="citation_title=Cellpose: A generalist algorithm for cellular segmentation;,citation_author=Carsen Stringer;,citation_author=Tim Wang;,citation_author=Michalis Michaelos;,citation_author=Marius Pachitariu;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://doi.org/10.1038/s41592-020-01018-x;,citation_issue=1;,citation_doi=10.1038/s41592-020-01018-x;,citation_issn=1548-7105;,citation_volume=18;,citation_journal_title=Nature Methods;">
<meta name="citation_reference" content="citation_title=Cellpose 2.0: How to train your own model;,citation_author=Marius Pachitariu;,citation_author=Carsen Stringer;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://doi.org/10.1038/s41592-022-01663-4;,citation_issue=12;,citation_doi=10.1038/s41592-022-01663-4;,citation_issn=1548-7105;,citation_volume=19;,citation_journal_title=Nature Methods;">
<meta name="citation_reference" content="citation_title=Cellpose3: One-click image restoration for improved cellular segmentation;,citation_abstract=Generalist methods for cellular segmentation have good out-of-the-box performance on a variety of image types; however, existing methods struggle for images that are degraded by noise, blurring or undersampling, all of which are common in microscopy. We focused the development of Cellpose3 on addressing these cases and here we demonstrate substantial out-of-the-box gains in segmentation and image quality for noisy, blurry and undersampled images. Unlike previous approaches that train models to restore pixel values, we trained Cellpose3 to output images that are well segmented by a generalist segmentation model, while maintaining perceptual similarity to the target images. Furthermore, we trained the restoration models on a large, varied collection of datasets, thus ensuring good generalization to user images. We provide these tools as “one-click” buttons inside the graphical interface of Cellpose as well as in the Cellpose API.;,citation_author=Carsen Stringer;,citation_author=Marius Pachitariu;,citation_publication_date=2025-03;,citation_cover_date=2025-03;,citation_year=2025;,citation_fulltext_html_url=https://doi.org/10.1038/s41592-025-02595-5;,citation_issue=3;,citation_doi=10.1038/s41592-025-02595-5;,citation_issn=1548-7105;,citation_volume=22;,citation_journal_title=Nature Methods;">
<meta name="citation_reference" content="citation_title=Deep learning-enabled segmentation of ambiguous bioimages with deepflash2;,citation_author=Matthias Griebel;,citation_author=Dennis Segebarth;,citation_author=Nikolai Stein;,citation_author=Nina Schukraft;,citation_author=Philip Tovote;,citation_author=Robert Blum;,citation_author=Christoph M. Flath;,citation_publication_date=2023-02;,citation_cover_date=2023-02;,citation_year=2023;,citation_fulltext_html_url=https://doi.org/10.5281/zenodo.7653312;,citation_doi=10.5281/zenodo.7653312;,citation_publisher=Zenodo;">
<meta name="citation_reference" content="citation_title=An annotated fluorescence image dataset for training nuclear segmentation methods;,citation_author=Peter F. Ambros Sabine Taschner-Mandl;,citation_author=Florian Kromp;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BSST265;">
<meta name="citation_reference" content="citation_title=Fluorescent neuronal cells v2;,citation_abstract=Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Science and Deep Learning.This dataset encompasses three image collections wherein rodent neuronal cell nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Specifically, we release 1874 high-resolution images alongside 750 corresponding ground-truth annotations for several learning tasks, including semantic segmentation, object detection and counting.The contribution is two-fold. First, thanks to the variety of annotations and their accessible formats, we envision our work would facilitate methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 would catalyze breakthroughs in fluorescence microscopy analysis and promote cutting-edge discoveries in life sciences.For more information, please refer to Clissa, L. et al., 2024. Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy. Scientific data. https://doi.org/10.1038/s41597-024-03005-9.This research was partly funded by PNRR - M4C2 - Investimento 1.3, Partenariato Esteso PE00000013 - &amp;amp;amp;quot;FAIR - Future Artificial Intelligence Research&amp;quot; - Spoke 8 &quot;Pervasive AI&quot; and the European Commission under the NextGeneration EU programme. The collection of original images was supported by funding from the University of Bologna and the European Space Agency (Research agreement collaboration 4000123556).;,citation_author=Luca Clissa;,citation_author=Alessandra Occhinegro;,citation_author=Emiliana Piscitiello;,citation_author=Ludovico Taddei;,citation_author=Antonio Macaluso;,citation_author=Roberto Morelli;,citation_author=Fabio Squarcio;,citation_author=Timna Hitrec;,citation_author=Alessia Di Cristoforo;,citation_author=Marco Luppi;,citation_author=Roberto Amici;,citation_author=Matteo Cerri;,citation_author=Stefano Bastianini;,citation_author=Chiara Berteotti;,citation_author=Viviana Lo Martire;,citation_author=Davide Martelli;,citation_author=Domenico Tupone;,citation_author=Giovanna Zoccoli;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://amsacta.unibo.it/id/eprint/7347/;,citation_publisher=University of Bologna;">
<meta name="citation_reference" content="citation_title=BBBC007v1: Drosophila Kc167 RNAi screening dataset;,citation_author=T. R. Jones;,citation_author=A. E. Carpenter;,citation_author=others;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_publisher=https://bbbc.broadinstitute.org/BBBC007;">
<meta name="citation_reference" content="citation_title=BBBC009v1: Human red blood cells DIC dataset;,citation_author=R. Wiegand;,citation_author=A. E. Carpenter;,citation_author=others;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_publisher=https://bbbc.broadinstitute.org/BBBC009;">
<meta name="citation_reference" content="citation_title=BBBC030v1: DIC image reconstruction dataset from the broad bioimage benchmark collection;,citation_author=K. Koos;,citation_author=J. Molnár;,citation_author=L. Kelemen;,citation_author=G. Tamás;,citation_author=P. Horvath;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_publisher=https://bbbc.broadinstitute.org/BBBC030;">
<meta name="citation_reference" content="citation_title=BBBC034v1: Allen institute cell structure dataset;,citation_author=D. Thirstrup;,citation_author=others;,citation_publication_date=2018;,citation_cover_date=2018;,citation_year=2018;,citation_publisher=https://bbbc.broadinstitute.org/BBBC034;">
<meta name="citation_reference" content="citation_title=BBBC038v1: 2018 data science bowl nucleus segmentation dataset;,citation_author=J. C. Caicedo;,citation_author=A. Goodman;,citation_author=K. W. Karhohs;,citation_author=others;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_publisher=https://bbbc.broadinstitute.org/BBBC038;">
<meta name="citation_reference" content="citation_title=BBBC039v1: Fluorescence nucleus segmentation dataset;,citation_author=J. C. Caicedo;,citation_author=J. Roth;,citation_author=A. Goodman;,citation_author=T. Becker;,citation_author=K. W. Karhohs;,citation_author=M. Broisin;,citation_author=C. Molnar;,citation_author=C. McQuin;,citation_author=S. Singh;,citation_author=F. J. Theis;,citation_author=A. E. Carpenter;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_publisher=https://bbbc.broadinstitute.org/BBBC039;">
<meta name="citation_reference" content="citation_title=CCDB:6843 - mus musculus neuroblastoma dataset;,citation_author=W. Yu;,citation_author=H. K. Lee;,citation_author=S. Hariharan;,citation_author=W. Y. Bu;,citation_author=S. Ahmed;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_publisher=https://www.cellimagelibrary.org/images/CCDB_6843;">
<meta name="citation_reference" content="citation_title=Objective criteria for the evaluation of clustering methods;,citation_author=William M. Rand;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_fulltext_html_url= 
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1971.10482356
;,citation_issue=336;,citation_doi=10.1080/01621459.1971.10482356;,citation_volume=66;,citation_journal_title=Journal of the American Statistical Association;,citation_publisher=ASA Website;">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../.././assets/logo_Bruker_white.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../license.html"> 
<span class="menu-text">License</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../link-to-code.html"> 
<span class="menu-text">Code</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">Browse Posts by Topic</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/company/bruker-spatial"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-right">
      <h1 class="title">Exporting <code>napari-cosmx</code> Zarr stores to OME-TIFF</h1>
                  <div>
        <div class="description">
          This post shows you how to generate OME-TIFF files from <code>napari-cosmx</code>-generated Zarr stores.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">napari</div>
                <div class="quarto-category">how-tos</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">OME-TIFF</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author column-page-right">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliations</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Evelyn Metzger <a href="https://orcid.org/0000-0002-4074-9003" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              Bruker Spatial Biology
            </p>
          <p class="affiliation">
              Github: <a href="https://github.com/eveilyeverafter" target="_blank">eveilyeverafter</a>
            </p>
        </div>
    </div>

  <div class="quarto-title-meta column-page-right">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 21, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="2">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#installation" id="toc-installation" class="nav-link" data-scroll-target="#installation"><span class="header-section-number">2</span> Installation</a></li>
  <li><a href="#usage" id="toc-usage" class="nav-link" data-scroll-target="#usage"><span class="header-section-number">3</span> Usage</a></li>
  <li><a href="#human-6k-lymph-node-example" id="toc-human-6k-lymph-node-example" class="nav-link" data-scroll-target="#human-6k-lymph-node-example"><span class="header-section-number">4</span> Human 6K Lymph Node Example</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#installing-optional-dependency" id="toc-installing-optional-dependency" class="nav-link" data-scroll-target="#installing-optional-dependency">Installing optional dependency</a>
  <ul class="collapse">
  <li><a href="#running-with---libvips" id="toc-running-with---libvips" class="nav-link" data-scroll-target="#running-with---libvips">Running with <code>--libvips</code></a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-right" id="quarto-document-content">





<div class="cell">
<div class="cell-output-display">
<div id="fig-gc" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./figures/fig-gc.png" class="img-fluid figure-img" width="769">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gc-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: A regressed germinal center within a lymph node sample visualized with the <code>napari-cosmx</code> plugin. Membrane = Grey; DAPI = blue, cell segementation = cyan.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Immunofluorescence (IF) and protein data exported from AtoMx<sup>®</sup> Spatial Informatics Platform (SIP) are provided as multi-channel TIFF files, where each field of view (FOV) is contained in a separate file. Our <code>napari-cosmx</code> plugin offers one method to stitch these individual TIFF files, organizing them according to their spatial location. This stitching process within <code>napari-cosmx</code> generates a Zarr store for each IF or protein image. For visualization of tissue microscopy data, <a href="https://docs.openmicroscopy.org/ome-model/5.6.3/ome-tiff/">OME-TIFF</a> is a widely used format. The Zarr stores generated with <code>napari-cosmx</code> can be converted to OME-TIFF for viewing in other applications and this short blog post demonstrates how to perform this conversion.</p>
<p>This blog post is the fifth in our <a href="https://nanostring-biostats.github.io/CosMx-Analysis-Scratch-Space/#category=napari"><code>napari series</code></a>. For details on generating these napari Zarr stores, refer to either the GUI-based <a href="../../posts/napari-cosmx-intro/index.html">napari-cosmx plugin introduction</a> or the command-line tutorial on <a href="../../posts/napari-stitching/napari-cosmx-stitching.html">stitching</a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While Napari and <code>napari-cosmx</code> can be installed on many systems, CosMx SMI data can be quite large. A slide with numerous FOVs may exceed the capabilities of a standard laptop.</p>
<p><code>napari-cosmx</code> is actively and continuously under development in the RnD groups at Bruker Spatial Biology. We do not (yet) have the source code for <code>napari-cosmx</code> opened-source. Please be aware that there may be bugs and that it has not gone through the regular level of quality and testing. Our goal here is to bring the capabilities of napari to CosMx SMI users as fast as possible.</p>
</div>
</div>
</section>
<section id="installation" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Installation</h1>
<p>This tutorial uses version 0.4.17.3 of the <code>napari-cosmx</code> plugin. This version is available as a <code>whl</code> file in the assets folder of the <a href="https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/blob/Main/assets/napari-cosmx%20releases/napari_CosMx-0.4.17.3-py3-none-any.whl">Scratch Space repository</a>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It is encouraged to use a virtual environment for these steps as there can be conflicting package and Python versions and this script has not been installed nor tested on all configurations. The example below used Python 3.9.13 within <code>pyenv</code>. In general, it is recommended to use Python &gt; 3.8 and Python &lt; 3.11 for the <code>napari-cosmx</code> plugin as newer versions of napari are not yet integrated into the plugin.</p>
</div>
</div>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">MacOS/Unix Install</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Windows</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<p>Assuming you have a <code>pyenv</code> virtual environment named napari_env:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># terminal</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> activate napari_env</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="st">"napari[all]"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional use wget to download whl file directly.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># wget https://github.com/Nanostring-Biostats/CosMx-Analysis-Scratch-Space/raw/refs/heads/Main/assets/napari-cosmx%20releases/napari_CosMx-0.4.17.3-py3-none-any.whl</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install /path/to/your/whl/file</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>You can confirm that the install worked with <code>pip freeze</code> or by directly accessing the package script:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span> export-tiff</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which should return the path to <code>export-tiff</code>.</p>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<p>For windows, <a href="https://github.com/pyenv-win/pyenv-win"><code>pyenv-win</code></a> and <a href="https://github.com/pyenv-win/pyenv-win-venv"><code>pyenv-win-venv</code></a> may be a good option but this has not been extensively tested.</p>
<p>Assuming you have a <code>pyenv-venv</code> virtual environment named napari_env:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># terminal</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv-venv</span> activate napari_env</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="st">"napari[all]"</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install /path/to/your/whl/file</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</section>
<section id="usage" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Usage</h1>
<p>For help, type <code>export-tiff --help</code> in your terminal (unix/macOS) or PowerShell (Windows).</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">export-tiff</span> <span class="at">--help</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>usage: export-tiff [-h] [-i INPUTDIR] [-o OUTPUTDIR] [--filename FILENAME] [--compression COMPRESSION] [-b BATCHSIZE]
                   [-s] [-c [CHANNELS ...]] [-p [PROTEINS ...]] [--levels LEVELS] [-v] [--libvips]
                   [--vipshome VIPSHOME] [--vipsconcurrency VIPSCONCURRENCY]

Export stitched Zarr to OME-TIFF

optional arguments:
  -h, --help            show this help message and exit
  -i INPUTDIR, --inputdir INPUTDIR
                        Required: Path to existing stitched output.
  -o OUTPUTDIR, --outputdir OUTPUTDIR
                        Required: Path to write OME-TIFF file.
  --filename FILENAME   Name for OME-TIFF file, use ome.tif extension.
  --compression COMPRESSION
                        Passed to TiffWriter, default is 'zlib'. Other options include 'lzma' (smallest), 'lzw', and
                        'none'
  -b BATCHSIZE, --batchsize BATCHSIZE
                        Required: the number of elements to put into each ome-tiff file. Recommended = 5 or fewer.
  -s, --segmentation    Optional: Create TIFF for segmentation mask.
  -c [CHANNELS ...], --channels [CHANNELS ...]
                        Optional: Output only specific morphology channels
  -p [PROTEINS ...], --proteins [PROTEINS ...]
                        Optional: Output only specific proteins
  --levels LEVELS       Optional: Specify number of pyramid levels.
  -v, --verbose         Print verbose output?
  --libvips             Optional: Use libvips to create pyramidal image, will be slower but more memory-efficient.
  --vipshome VIPSHOME   Optional: Path to vips binaries. Required in Windows if vips and associated DLLs are not in
                        PATH
  --vipsconcurrency VIPSCONCURRENCY
                        Optional: Specify number of threads for vips.</code></pre>
<p>Since CosMx SMI data with hundreds of FOVs can be large (<em>e.g.</em>, &gt;50 GBs for RNA and larger still for protein data), the <code>--batchsize</code> flag can be used to create batches of smaller OME-TIFF files. The <code>--batchsize</code> flag specifies the upper limit of the number of channels/proteins that are included in a given OME-TIFF. For example, if <code>--batchsize</code> was set to <code>5</code>, and <code>-c</code> (channels) was also selected, the OME-TIFF file would include all five IF channels.</p>
<p>Note that the segmentation (<code>-s</code>) layer does not count toward the batch size. In other words, if selecting <code>-b 5 -c -s</code>, the resulting OME-TIFF file will include the cell borders and the five channels.</p>
<p>If you would like to generate an OME-TIFF file with only DNA, GFAP, and proteins Amyloid-Beta-1-40 and Phospho-Tau-S199, with cell borders:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">export-tiff</span> <span class="at">-i</span> /path/to/images/folder <span class="at">-o</span> /path/to/output/folder <span class="at">-s</span> <span class="at">-c</span> DNA,GFAP <span class="at">-p</span> Amyloid-Beta-1-40,Phospho-Tau-S199</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="human-6k-lymph-node-example" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Human 6K Lymph Node Example</h1>
<p>Let’s test this script with an example. Here I used the Human Lymph Node FFPE dataset napari files that are <a href="https://nanostring.com/products/cosmx-spatial-molecular-imager/ffpe-dataset/cosmx-human-lymph-node-ffpe-dataset/">publicly available</a>. This dataset has a scan area of 104 <span class="math inline">\(mm^2\)</span>, 400 FOVs, and ~1.8M cells. I am using an AWS r5b.8xlarge EC2 instance (32 vCPUs, 256 GB RAM) running Linux (Amazon Linux 2).</p>
<p>Navigate to the location of <code>napari.zip</code> and unzip it.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Terminal</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">unzip</span> napari.zip</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To convert all the channels and the cell segmentation boundaries:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Terminal</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TIME="%e %U %S" time export-tiff -i ./napari -o output -s -c -b 5</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">export-tiff</span> <span class="at">-i</span> ./napari <span class="at">-o</span> output <span class="at">-s</span> <span class="at">-c</span> <span class="at">-b</span> 5</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>5352.18 50467.38 11061.83 10366568</p>
</blockquote>
<p>This script converted a single OME-TIFF file in just under 1.5 hours (5352/3600 = 1.49).</p>
<p>The size of the OME-TIFF file, which contains five IF channels plus the segmentation layer, is 54 GBs (<em>i.e.</em>, about 91% of the size of the Zarr stores).</p>
<p>The default method to convert Zarr to OME-TIFF can require more memory than your system has. If you run into memory issues, you can try the secondary method that uses <code>pyvips</code> (see Appendix).</p>
</section>
<section id="appendix" class="level1 unnumbered">
<h1 class="unnumbered">Appendix</h1>
<p>The <code>export-tiff</code> utility script runs in two different modes. The first one (default) is generally the quicker of the two. However, if you are running into memory issues, you can try the second option that is memory-optimized but runs slower and requires more disk space.</p>
<section id="installing-optional-dependency" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="installing-optional-dependency">Installing optional dependency</h2>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">MacOS/Unix Install</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Windows</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<p>For macOS, I have had success with using brew following libvips’ <a href="https://www.libvips.org/install.html">recommendation</a>.</p>
<p>For Linux, be sure that the appropriate libraries are installed for your distribution. Check out libvips’ <a href="https://github.com/libvips/libvips/wiki/#building-and-installing">github wiki</a> for more tips.</p>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<p>Download and install <code>vips-dev-w64-all-8.16.0.zip</code> from <a href="https://github.com/libvips/build-win64-mxe/releases/">here</a></p>
<p>Unzip it and move its contents to your desired location. Add the folder to a convenient location and add the location of the binaries (<em>e.g.</em>, <code>C:\vips-dev-8.16.0\bin</code>) to your Path. Restart your computer.</p>
<p>When using the <code>--libvips</code> flag within the <code>export-tiff</code> package script, python will look for these binaries.</p>
</div>
</div>
</div>
<section id="running-with---libvips" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="running-with---libvips">Running with <code>--libvips</code></h3>
<p>Rerunning the previous Lymph Node example with <code>--libvips</code>:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Terminal</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># TIME="%e %U %S" time export-tiff -i ./napari -o output -s -c -b 5</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="ex">export-tiff</span> <span class="at">-i</span> ./napari <span class="at">-o</span> output <span class="at">-s</span> <span class="at">-c</span> <span class="at">-b</span> 5 <span class="at">--libvips</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<blockquote class="blockquote">
<p>16894.29 16987.47 438.18 16905396</p>
</blockquote>
<p>or about 4.6 hours.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/nanostring-biostats\.github\.io\/CosMx-Analysis-Scratch-Space");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="../../license.html">
<p>License</p>
</a>
  </li>  
</ul>
    <div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>